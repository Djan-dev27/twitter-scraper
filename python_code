#twitter-scraper.py

import time
import json
import os
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException, StaleElementReferenceException
from webdriver_manager.chrome import ChromeDriverManager

def get_latest_posts(username, num_posts=10):
    # Setting up the Chrome WebDriver
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    # Open Twitter profile
    driver.get(f"https://twitter.com/{username}")
    time.sleep(5)  

    posts_data = []
    try:
        # Wait for the first tweet to be present
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, 'article'))
        )

        # Scroll and collect tweets
        last_height = driver.execute_script("return document.body.scrollHeight")
        while len(posts_data) < num_posts:
            tweets = driver.find_elements(By.CSS_SELECTOR, 'article')
            for tweet in tweets[len(posts_data):num_posts]:
                post_data = {}
                try:
                    # Extract tweet text
                    post_data['text'] = tweet.find_element(By.CSS_SELECTOR, 'div[lang]').text

                    # Extract tweet timestamp
                    post_data['time'] = tweet.find_element(By.TAG_NAME, 'time').get_attribute('datetime')

                    # Extract images (if any)
                    images = []
                    image_elements = tweet.find_elements(By.CSS_SELECTOR, 'img[src*="media"]')
                    for img in image_elements:
                        images.append(img.get_attribute('src'))
                    post_data['images'] = images

                    # Extract videos (if any)
                    videos = []
                    video_elements = tweet.find_elements(By.CSS_SELECTOR, 'video')
                    for vid in video_elements:
                        video_src = vid.get_attribute('src')
                        if not video_src:
                            # Try to get the video source from the video tag's child elements
                            video_src = vid.find_element(By.TAG_NAME, 'source').get_attribute('src')
                        if video_src:
                            videos.append(video_src)
                    post_data['videos'] = videos

                    posts_data.append(post_data)

                except NoSuchElementException:
                    continue
                except StaleElementReferenceException:
                    # Re-fetch the tweet elements if they become stale
                    tweets = driver.find_elements(By.CSS_SELECTOR, 'article')
                    continue

            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

    except TimeoutException:
        print(f"Timeout while waiting for tweets from {username}")

    finally:
        driver.quit()

    return posts_data

def save_posts_to_file(posts_data, username):
    filename = f"{username}_latest_posts.txt"
    if os.path.exists(filename):
        with open(filename, 'r') as file:
            existing_data = json.load(file)
            existing_data.extend(posts_data)
        with open(filename, 'w') as file:
            json.dump(existing_data, file, indent=4)
    else:
        with open(filename, 'w') as file:
            json.dump(posts_data, file, indent=4)

if __name__ == "__main__":
    while True:
        username = input("Enter the Twitter username (or 'exit' to stop): ")
        if username.lower() == 'exit':
            break
        latest_posts = get_latest_posts(username)
        if latest_posts:
            save_posts_to_file(latest_posts, username)
            print(f"Saved the latest posts of {username} to {username}_latest_posts.json")
        else:
            print(f"No posts found or error occurred during scraping for {username}")
